// RUN: %optcall --vectorize-lospn-nodes %s | FileCheck %s
// Test (re)generated by regenerate_tests.py.
// NOTE: Assertions have been autogenerated by utils/generate-test-checks.py

// The script is designed to make adding checks to
// a test case fast, it is *not* designed to be authoritative
// about what constitutes a good test! The CHECK should be
// minimized and named to reflect the test intent.

// RUN: %optcall --vectorize-lospn-nodes %s | FileCheck %s
module {
// CHECK-LABEL:   func.func @vec_task_0(
// CHECK-SAME:                          %[[VAL_0:.*]]: memref<?x4xf32>,
// CHECK-SAME:                          %[[VAL_1:.*]]: memref<1x?xf64>) {
// CHECK:           %[[VAL_2:.*]] = arith.constant 0 : index
// CHECK:           %[[VAL_3:.*]] = memref.dim %[[VAL_0]], %[[VAL_2]] : memref<?x4xf32>
// CHECK:           %[[VAL_4:.*]] = arith.constant 8 : index
// CHECK:           %[[VAL_5:.*]] = arith.remui %[[VAL_3]], %[[VAL_4]] : index
// CHECK:           %[[VAL_6:.*]] = arith.subi %[[VAL_3]], %[[VAL_5]] : index
// CHECK:           %[[VAL_7:.*]] = arith.constant 0 : index
// CHECK:           %[[VAL_8:.*]] = arith.constant 8 : index
// CHECK:           scf.for %[[VAL_9:.*]] = %[[VAL_7]] to %[[VAL_6]] step %[[VAL_8]] {
// CHECK:             %[[VAL_10:.*]] = arith.index_cast %[[VAL_9]] : index to i64
// CHECK:             %[[VAL_11:.*]] = vector.broadcast %[[VAL_10]] : i64 to vector<4xi64>
// CHECK:             %[[VAL_12:.*]] = arith.constant dense<[0, 4, 8, 12]> : vector<4xi64>
// CHECK:             %[[VAL_13:.*]] = arith.constant dense<4> : vector<4xi64>
// CHECK:             %[[VAL_14:.*]] = arith.muli %[[VAL_11]], %[[VAL_13]] : vector<4xi64>
// CHECK:             %[[VAL_15:.*]] = arith.addi %[[VAL_14]], %[[VAL_12]] : vector<4xi64>
// CHECK:             %[[VAL_16:.*]] = arith.constant dense<0.000000e+00> : vector<4xf32>
// CHECK:             %[[VAL_17:.*]] = arith.constant dense<true> : vector<4xi1>
// CHECK:             %[[VAL_18:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_19:.*]] = memref.dim %[[VAL_0]], %[[VAL_18]] : memref<?x4xf32>
// CHECK:             %[[VAL_20:.*]] = arith.constant 4 : index
// CHECK:             %[[VAL_21:.*]] = arith.muli %[[VAL_19]], %[[VAL_20]] : index
// CHECK:             %[[VAL_22:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: {{\[}}%[[VAL_21]]], strides: [1] : memref<?x4xf32> to memref<?xf32>
// CHECK:             %[[VAL_23:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_24:.*]] = vector.gather %[[VAL_22]]{{\[}}%[[VAL_23]]] {{\[}}%[[VAL_15]]], %[[VAL_17]], %[[VAL_16]] : memref<?xf32>, vector<4xi64>, vector<4xi1>, vector<4xf32> into vector<4xf32>
// CHECK:             %[[VAL_25:.*]] = arith.index_cast %[[VAL_9]] : index to i64
// CHECK:             %[[VAL_26:.*]] = vector.broadcast %[[VAL_25]] : i64 to vector<4xi64>
// CHECK:             %[[VAL_27:.*]] = arith.constant dense<[1, 5, 9, 13]> : vector<4xi64>
// CHECK:             %[[VAL_28:.*]] = arith.constant dense<4> : vector<4xi64>
// CHECK:             %[[VAL_29:.*]] = arith.muli %[[VAL_26]], %[[VAL_28]] : vector<4xi64>
// CHECK:             %[[VAL_30:.*]] = arith.addi %[[VAL_29]], %[[VAL_27]] : vector<4xi64>
// CHECK:             %[[VAL_31:.*]] = arith.constant dense<0.000000e+00> : vector<4xf32>
// CHECK:             %[[VAL_32:.*]] = arith.constant dense<true> : vector<4xi1>
// CHECK:             %[[VAL_33:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_34:.*]] = memref.dim %[[VAL_0]], %[[VAL_33]] : memref<?x4xf32>
// CHECK:             %[[VAL_35:.*]] = arith.constant 4 : index
// CHECK:             %[[VAL_36:.*]] = arith.muli %[[VAL_34]], %[[VAL_35]] : index
// CHECK:             %[[VAL_37:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: {{\[}}%[[VAL_36]]], strides: [1] : memref<?x4xf32> to memref<?xf32>
// CHECK:             %[[VAL_38:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_39:.*]] = vector.gather %[[VAL_37]]{{\[}}%[[VAL_38]]] {{\[}}%[[VAL_30]]], %[[VAL_32]], %[[VAL_31]] : memref<?xf32>, vector<4xi64>, vector<4xi1>, vector<4xf32> into vector<4xf32>
// CHECK:             %[[VAL_40:.*]] = arith.index_cast %[[VAL_9]] : index to i64
// CHECK:             %[[VAL_41:.*]] = vector.broadcast %[[VAL_40]] : i64 to vector<4xi64>
// CHECK:             %[[VAL_42:.*]] = arith.constant dense<[2, 6, 10, 14]> : vector<4xi64>
// CHECK:             %[[VAL_43:.*]] = arith.constant dense<4> : vector<4xi64>
// CHECK:             %[[VAL_44:.*]] = arith.muli %[[VAL_41]], %[[VAL_43]] : vector<4xi64>
// CHECK:             %[[VAL_45:.*]] = arith.addi %[[VAL_44]], %[[VAL_42]] : vector<4xi64>
// CHECK:             %[[VAL_46:.*]] = arith.constant dense<0.000000e+00> : vector<4xf32>
// CHECK:             %[[VAL_47:.*]] = arith.constant dense<true> : vector<4xi1>
// CHECK:             %[[VAL_48:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_49:.*]] = memref.dim %[[VAL_0]], %[[VAL_48]] : memref<?x4xf32>
// CHECK:             %[[VAL_50:.*]] = arith.constant 4 : index
// CHECK:             %[[VAL_51:.*]] = arith.muli %[[VAL_49]], %[[VAL_50]] : index
// CHECK:             %[[VAL_52:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: {{\[}}%[[VAL_51]]], strides: [1] : memref<?x4xf32> to memref<?xf32>
// CHECK:             %[[VAL_53:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_54:.*]] = vector.gather %[[VAL_52]]{{\[}}%[[VAL_53]]] {{\[}}%[[VAL_45]]], %[[VAL_47]], %[[VAL_46]] : memref<?xf32>, vector<4xi64>, vector<4xi1>, vector<4xf32> into vector<4xf32>
// CHECK:             %[[VAL_55:.*]] = arith.index_cast %[[VAL_9]] : index to i64
// CHECK:             %[[VAL_56:.*]] = vector.broadcast %[[VAL_55]] : i64 to vector<4xi64>
// CHECK:             %[[VAL_57:.*]] = arith.constant dense<[3, 7, 11, 15]> : vector<4xi64>
// CHECK:             %[[VAL_58:.*]] = arith.constant dense<4> : vector<4xi64>
// CHECK:             %[[VAL_59:.*]] = arith.muli %[[VAL_56]], %[[VAL_58]] : vector<4xi64>
// CHECK:             %[[VAL_60:.*]] = arith.addi %[[VAL_59]], %[[VAL_57]] : vector<4xi64>
// CHECK:             %[[VAL_61:.*]] = arith.constant dense<0.000000e+00> : vector<4xf32>
// CHECK:             %[[VAL_62:.*]] = arith.constant dense<true> : vector<4xi1>
// CHECK:             %[[VAL_63:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_64:.*]] = memref.dim %[[VAL_0]], %[[VAL_63]] : memref<?x4xf32>
// CHECK:             %[[VAL_65:.*]] = arith.constant 4 : index
// CHECK:             %[[VAL_66:.*]] = arith.muli %[[VAL_64]], %[[VAL_65]] : index
// CHECK:             %[[VAL_67:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: {{\[}}%[[VAL_66]]], strides: [1] : memref<?x4xf32> to memref<?xf32>
// CHECK:             %[[VAL_68:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_69:.*]] = vector.gather %[[VAL_67]]{{\[}}%[[VAL_68]]] {{\[}}%[[VAL_60]]], %[[VAL_62]], %[[VAL_61]] : memref<?xf32>, vector<4xi64>, vector<4xi1>, vector<4xf32> into vector<4xf32>
// CHECK:             %[[VAL_70:.*]] = arith.extf %[[VAL_24]] : vector<4xf32> to vector<4xf64>
// CHECK:             %[[VAL_71:.*]] = arith.constant dense<-5.000000e-01> : vector<4xf64>
// CHECK:             %[[VAL_72:.*]] = arith.constant dense<-0.91893853320467266> : vector<4xf64>
// CHECK:             %[[VAL_73:.*]] = arith.constant dense<1.100000e-01> : vector<4xf64>
// CHECK:             %[[VAL_74:.*]] = arith.subf %[[VAL_70]], %[[VAL_73]] : vector<4xf64>
// CHECK:             %[[VAL_75:.*]] = arith.mulf %[[VAL_74]], %[[VAL_74]] : vector<4xf64>
// CHECK:             %[[VAL_76:.*]] = arith.mulf %[[VAL_75]], %[[VAL_71]] : vector<4xf64>
// CHECK:             %[[VAL_77:.*]] = arith.addf %[[VAL_72]], %[[VAL_76]] : vector<4xf64>
// CHECK:             %[[VAL_78:.*]] = arith.extf %[[VAL_39]] : vector<4xf32> to vector<4xf64>
// CHECK:             %[[VAL_79:.*]] = arith.constant dense<-0.88888888888888883> : vector<4xf64>
// CHECK:             %[[VAL_80:.*]] = arith.constant dense<-0.63125646075289177> : vector<4xf64>
// CHECK:             %[[VAL_81:.*]] = arith.constant dense<1.200000e-01> : vector<4xf64>
// CHECK:             %[[VAL_82:.*]] = arith.subf %[[VAL_78]], %[[VAL_81]] : vector<4xf64>
// CHECK:             %[[VAL_83:.*]] = arith.mulf %[[VAL_82]], %[[VAL_82]] : vector<4xf64>
// CHECK:             %[[VAL_84:.*]] = arith.mulf %[[VAL_83]], %[[VAL_79]] : vector<4xf64>
// CHECK:             %[[VAL_85:.*]] = arith.addf %[[VAL_80]], %[[VAL_84]] : vector<4xf64>
// CHECK:             %[[VAL_86:.*]] = arith.extf %[[VAL_54]] : vector<4xf32> to vector<4xf64>
// CHECK:             %[[VAL_87:.*]] = arith.constant dense<-2.000000e+00> : vector<4xf64>
// CHECK:             %[[VAL_88:.*]] = arith.constant dense<-0.22579135264472738> : vector<4xf64>
// CHECK:             %[[VAL_89:.*]] = arith.constant dense<1.300000e-01> : vector<4xf64>
// CHECK:             %[[VAL_90:.*]] = arith.subf %[[VAL_86]], %[[VAL_89]] : vector<4xf64>
// CHECK:             %[[VAL_91:.*]] = arith.mulf %[[VAL_90]], %[[VAL_90]] : vector<4xf64>
// CHECK:             %[[VAL_92:.*]] = arith.mulf %[[VAL_91]], %[[VAL_87]] : vector<4xf64>
// CHECK:             %[[VAL_93:.*]] = arith.addf %[[VAL_88]], %[[VAL_92]] : vector<4xf64>
// CHECK:             %[[VAL_94:.*]] = arith.extf %[[VAL_69]] : vector<4xf32> to vector<4xf64>
// CHECK:             %[[VAL_95:.*]] = arith.constant dense<-8.000000e+00> : vector<4xf64>
// CHECK:             %[[VAL_96:.*]] = arith.constant dense<0.4673558279152179> : vector<4xf64>
// CHECK:             %[[VAL_97:.*]] = arith.constant dense<1.400000e-01> : vector<4xf64>
// CHECK:             %[[VAL_98:.*]] = arith.subf %[[VAL_94]], %[[VAL_97]] : vector<4xf64>
// CHECK:             %[[VAL_99:.*]] = arith.mulf %[[VAL_98]], %[[VAL_98]] : vector<4xf64>
// CHECK:             %[[VAL_100:.*]] = arith.mulf %[[VAL_99]], %[[VAL_95]] : vector<4xf64>
// CHECK:             %[[VAL_101:.*]] = arith.addf %[[VAL_96]], %[[VAL_100]] : vector<4xf64>
// CHECK:             %[[VAL_102:.*]] = arith.constant dense<-1.3862943611198906> : vector<4xf64>
// CHECK:             %[[VAL_103:.*]] = arith.addf %[[VAL_77]], %[[VAL_102]] : vector<4xf64>
// CHECK:             %[[VAL_104:.*]] = arith.addf %[[VAL_85]], %[[VAL_102]] : vector<4xf64>
// CHECK:             %[[VAL_105:.*]] = arith.cmpf ogt, %[[VAL_103]], %[[VAL_104]] : vector<4xf64>
// CHECK:             %[[VAL_106:.*]] = arith.select %[[VAL_105]], %[[VAL_103]], %[[VAL_104]] : vector<4xi1>, vector<4xf64>
// CHECK:             %[[VAL_107:.*]] = arith.select %[[VAL_105]], %[[VAL_104]], %[[VAL_103]] : vector<4xi1>, vector<4xf64>
// CHECK:             %[[VAL_108:.*]] = arith.subf %[[VAL_107]], %[[VAL_106]] : vector<4xf64>
// CHECK:             %[[VAL_109:.*]] = math.exp %[[VAL_108]] : vector<4xf64>
// CHECK:             %[[VAL_110:.*]] = math.log1p %[[VAL_109]] : vector<4xf64>
// CHECK:             %[[VAL_111:.*]] = arith.addf %[[VAL_106]], %[[VAL_110]] : vector<4xf64>
// CHECK:             %[[VAL_112:.*]] = arith.addf %[[VAL_93]], %[[VAL_102]] : vector<4xf64>
// CHECK:             %[[VAL_113:.*]] = arith.addf %[[VAL_101]], %[[VAL_102]] : vector<4xf64>
// CHECK:             %[[VAL_114:.*]] = arith.cmpf ogt, %[[VAL_112]], %[[VAL_113]] : vector<4xf64>
// CHECK:             %[[VAL_115:.*]] = arith.select %[[VAL_114]], %[[VAL_112]], %[[VAL_113]] : vector<4xi1>, vector<4xf64>
// CHECK:             %[[VAL_116:.*]] = arith.select %[[VAL_114]], %[[VAL_113]], %[[VAL_112]] : vector<4xi1>, vector<4xf64>
// CHECK:             %[[VAL_117:.*]] = arith.subf %[[VAL_116]], %[[VAL_115]] : vector<4xf64>
// CHECK:             %[[VAL_118:.*]] = math.exp %[[VAL_117]] : vector<4xf64>
// CHECK:             %[[VAL_119:.*]] = math.log1p %[[VAL_118]] : vector<4xf64>
// CHECK:             %[[VAL_120:.*]] = arith.addf %[[VAL_115]], %[[VAL_119]] : vector<4xf64>
// CHECK:             %[[VAL_121:.*]] = arith.cmpf ogt, %[[VAL_111]], %[[VAL_120]] : vector<4xf64>
// CHECK:             %[[VAL_122:.*]] = arith.select %[[VAL_121]], %[[VAL_111]], %[[VAL_120]] : vector<4xi1>, vector<4xf64>
// CHECK:             %[[VAL_123:.*]] = arith.select %[[VAL_121]], %[[VAL_120]], %[[VAL_111]] : vector<4xi1>, vector<4xf64>
// CHECK:             %[[VAL_124:.*]] = arith.subf %[[VAL_123]], %[[VAL_122]] : vector<4xf64>
// CHECK:             %[[VAL_125:.*]] = math.exp %[[VAL_124]] : vector<4xf64>
// CHECK:             %[[VAL_126:.*]] = math.log1p %[[VAL_125]] : vector<4xf64>
// CHECK:             %[[VAL_127:.*]] = arith.addf %[[VAL_122]], %[[VAL_126]] : vector<4xf64>
// CHECK:             %[[VAL_128:.*]] = arith.constant 0 : index
// CHECK:             vector.transfer_write %[[VAL_127]], %[[VAL_1]]{{\[}}%[[VAL_128]], %[[VAL_9]]] : vector<4xf64>, memref<1x?xf64>
// CHECK:           }
// CHECK:           %[[VAL_129:.*]] = arith.constant 1 : index
// CHECK:           scf.for %[[VAL_130:.*]] = %[[VAL_6]] to %[[VAL_3]] step %[[VAL_129]] {
// CHECK:             %[[VAL_131:.*]] = "lo_spn.batch_read"(%[[VAL_0]], %[[VAL_130]]) <{staticIndex = 0 : ui32}> : (memref<?x4xf32>, index) -> f32
// CHECK:             %[[VAL_132:.*]] = "lo_spn.batch_read"(%[[VAL_0]], %[[VAL_130]]) <{staticIndex = 1 : ui32}> : (memref<?x4xf32>, index) -> f32
// CHECK:             %[[VAL_133:.*]] = "lo_spn.batch_read"(%[[VAL_0]], %[[VAL_130]]) <{staticIndex = 2 : ui32}> : (memref<?x4xf32>, index) -> f32
// CHECK:             %[[VAL_134:.*]] = "lo_spn.batch_read"(%[[VAL_0]], %[[VAL_130]]) <{staticIndex = 3 : ui32}> : (memref<?x4xf32>, index) -> f32
// CHECK:             %[[VAL_135:.*]] = "lo_spn.gaussian"(%[[VAL_131]]) <{mean = 1.100000e-01 : f64, stddev = 1.000000e+00 : f64, supportMarginal = false}> : (f32) -> !lo_spn.log<f64>
// CHECK:             %[[VAL_136:.*]] = "lo_spn.gaussian"(%[[VAL_132]]) <{mean = 1.200000e-01 : f64, stddev = 7.500000e-01 : f64, supportMarginal = false}> : (f32) -> !lo_spn.log<f64>
// CHECK:             %[[VAL_137:.*]] = "lo_spn.gaussian"(%[[VAL_133]]) <{mean = 1.300000e-01 : f64, stddev = 5.000000e-01 : f64, supportMarginal = false}> : (f32) -> !lo_spn.log<f64>
// CHECK:             %[[VAL_138:.*]] = "lo_spn.gaussian"(%[[VAL_134]]) <{mean = 1.400000e-01 : f64, stddev = 2.500000e-01 : f64, supportMarginal = false}> : (f32) -> !lo_spn.log<f64>
// CHECK:             %[[VAL_139:.*]] = "lo_spn.constant"() <{type = !lo_spn.log<f64>, value = -1.3862943611198906 : f64}> : () -> !lo_spn.log<f64>
// CHECK:             %[[VAL_140:.*]] = "lo_spn.mul"(%[[VAL_135]], %[[VAL_139]]) : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
// CHECK:             %[[VAL_141:.*]] = "lo_spn.mul"(%[[VAL_136]], %[[VAL_139]]) : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
// CHECK:             %[[VAL_142:.*]] = "lo_spn.add"(%[[VAL_140]], %[[VAL_141]]) : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
// CHECK:             %[[VAL_143:.*]] = "lo_spn.mul"(%[[VAL_137]], %[[VAL_139]]) : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
// CHECK:             %[[VAL_144:.*]] = "lo_spn.mul"(%[[VAL_138]], %[[VAL_139]]) : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
// CHECK:             %[[VAL_145:.*]] = "lo_spn.add"(%[[VAL_143]], %[[VAL_144]]) : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
// CHECK:             %[[VAL_146:.*]] = "lo_spn.add"(%[[VAL_142]], %[[VAL_145]]) : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
// CHECK:             %[[VAL_147:.*]] = "lo_spn.strip_log"(%[[VAL_146]]) <{target = f64}> : (!lo_spn.log<f64>) -> f64
// CHECK:             "lo_spn.batch_write"(%[[VAL_1]], %[[VAL_130]], %[[VAL_147]]) <{transposed = true}> : (memref<1x?xf64>, index, f64) -> ()
// CHECK:           }
// CHECK:           return
// CHECK:         }
  func.func @vec_task_0(%arg0: memref<?x4xf32>, %arg1: memref<1x?xf64>) {
    %c0 = arith.constant 0 : index
    %dim = memref.dim %arg0, %c0 : memref<?x4xf32>
    %c8 = arith.constant 8 : index
    %0 = arith.remui %dim, %c8 : index
    %1 = arith.subi %dim, %0 : index
    %c0_0 = arith.constant 0 : index
    %c8_1 = arith.constant 8 : index
    scf.for %arg2 = %c0_0 to %1 step %c8_1 {
      %2 = "lo_spn.batch_read"(%arg0, %arg2) <{staticIndex = 0 : ui32}> {vector_width = 4 : i32} : (memref<?x4xf32>, index) -> f32
      %3 = "lo_spn.batch_read"(%arg0, %arg2) <{staticIndex = 1 : ui32}> {vector_width = 4 : i32} : (memref<?x4xf32>, index) -> f32
      %4 = "lo_spn.batch_read"(%arg0, %arg2) <{staticIndex = 2 : ui32}> {vector_width = 4 : i32} : (memref<?x4xf32>, index) -> f32
      %5 = "lo_spn.batch_read"(%arg0, %arg2) <{staticIndex = 3 : ui32}> {vector_width = 4 : i32} : (memref<?x4xf32>, index) -> f32
      %6 = "lo_spn.gaussian"(%2) <{mean = 1.100000e-01 : f64, stddev = 1.000000e+00 : f64, supportMarginal = false}> {vector_width = 4 : i32} : (f32) -> !lo_spn.log<f64>
      %7 = "lo_spn.gaussian"(%3) <{mean = 1.200000e-01 : f64, stddev = 7.500000e-01 : f64, supportMarginal = false}> {vector_width = 4 : i32} : (f32) -> !lo_spn.log<f64>
      %8 = "lo_spn.gaussian"(%4) <{mean = 1.300000e-01 : f64, stddev = 5.000000e-01 : f64, supportMarginal = false}> {vector_width = 4 : i32} : (f32) -> !lo_spn.log<f64>
      %9 = "lo_spn.gaussian"(%5) <{mean = 1.400000e-01 : f64, stddev = 2.500000e-01 : f64, supportMarginal = false}> {vector_width = 4 : i32} : (f32) -> !lo_spn.log<f64>
      %10 = "lo_spn.constant"() <{type = !lo_spn.log<f64>, value = -1.3862943611198906 : f64}> {vector_width = 4 : i32} : () -> !lo_spn.log<f64>
      %11 = "lo_spn.mul"(%6, %10) {vector_width = 4 : i32} : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
      %12 = "lo_spn.mul"(%7, %10) {vector_width = 4 : i32} : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
      %13 = "lo_spn.add"(%11, %12) {vector_width = 4 : i32} : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
      %14 = "lo_spn.mul"(%8, %10) {vector_width = 4 : i32} : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
      %15 = "lo_spn.mul"(%9, %10) {vector_width = 4 : i32} : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
      %16 = "lo_spn.add"(%14, %15) {vector_width = 4 : i32} : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
      %17 = "lo_spn.add"(%13, %16) {vector_width = 4 : i32} : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
      %18 = "lo_spn.strip_log"(%17) <{target = f64}> {vector_width = 4 : i32} : (!lo_spn.log<f64>) -> f64
      "lo_spn.batch_write"(%arg1, %arg2, %18) <{transposed = true}> {vector_width = 4 : i32} : (memref<1x?xf64>, index, f64) -> ()
    }
    %c1 = arith.constant 1 : index
    scf.for %arg2 = %1 to %dim step %c1 {
      %2 = "lo_spn.batch_read"(%arg0, %arg2) <{staticIndex = 0 : ui32}> : (memref<?x4xf32>, index) -> f32
      %3 = "lo_spn.batch_read"(%arg0, %arg2) <{staticIndex = 1 : ui32}> : (memref<?x4xf32>, index) -> f32
      %4 = "lo_spn.batch_read"(%arg0, %arg2) <{staticIndex = 2 : ui32}> : (memref<?x4xf32>, index) -> f32
      %5 = "lo_spn.batch_read"(%arg0, %arg2) <{staticIndex = 3 : ui32}> : (memref<?x4xf32>, index) -> f32
      %6 = "lo_spn.gaussian"(%2) <{mean = 1.100000e-01 : f64, stddev = 1.000000e+00 : f64, supportMarginal = false}> : (f32) -> !lo_spn.log<f64>
      %7 = "lo_spn.gaussian"(%3) <{mean = 1.200000e-01 : f64, stddev = 7.500000e-01 : f64, supportMarginal = false}> : (f32) -> !lo_spn.log<f64>
      %8 = "lo_spn.gaussian"(%4) <{mean = 1.300000e-01 : f64, stddev = 5.000000e-01 : f64, supportMarginal = false}> : (f32) -> !lo_spn.log<f64>
      %9 = "lo_spn.gaussian"(%5) <{mean = 1.400000e-01 : f64, stddev = 2.500000e-01 : f64, supportMarginal = false}> : (f32) -> !lo_spn.log<f64>
      %10 = "lo_spn.constant"() <{type = !lo_spn.log<f64>, value = -1.3862943611198906 : f64}> : () -> !lo_spn.log<f64>
      %11 = "lo_spn.mul"(%6, %10) : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
      %12 = "lo_spn.mul"(%7, %10) : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
      %13 = "lo_spn.add"(%11, %12) : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
      %14 = "lo_spn.mul"(%8, %10) : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
      %15 = "lo_spn.mul"(%9, %10) : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
      %16 = "lo_spn.add"(%14, %15) : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
      %17 = "lo_spn.add"(%13, %16) : (!lo_spn.log<f64>, !lo_spn.log<f64>) -> !lo_spn.log<f64>
      %18 = "lo_spn.strip_log"(%17) <{target = f64}> : (!lo_spn.log<f64>) -> f64
      "lo_spn.batch_write"(%arg1, %arg2, %18) <{transposed = true}> : (memref<1x?xf64>, index, f64) -> ()
    }
    return
  }
// CHECK-LABEL:   func.func @spn_vector(
// CHECK-SAME:                          %[[VAL_0:.*]]: memref<?x4xf32>,
// CHECK-SAME:                          %[[VAL_1:.*]]: memref<1x?xf64>) {
// CHECK:           call @vec_task_0(%[[VAL_0]], %[[VAL_1]]) : (memref<?x4xf32>, memref<1x?xf64>) -> ()
// CHECK:           "lo_spn.return"() : () -> ()
// CHECK:         }
  func.func @spn_vector(%arg0: memref<?x4xf32>, %arg1: memref<1x?xf64>) {
    call @vec_task_0(%arg0, %arg1) : (memref<?x4xf32>, memref<1x?xf64>) -> ()
    "lo_spn.return"() : () -> ()
  }
}

