// RUN: %optcall -pass-pipeline="builtin.module(convert-lospn-structure-to-cpu{cpu-vectorize=true slp-max-attempts=1 slp-max-successful-iterations=1 slp-max-node-size=10 slp-max-look-ahead=5 slp-reorder-instructions-dfs=true slp-allow-duplicate-elements=false slp-allow-topological-mixing=false slp-use-xor-chains=true})" %s | FileCheck %s

module  {
  "lo_spn.kernel"() ( {
  ^bb0(%arg0: memref<?x4xi32>, %arg1: memref<1x?xf64>):  // no predecessors
    %c0 = arith.constant 0 : index
    %0 = memref.dim %arg0, %c0 : memref<?x4xi32>
    %1 = memref.alloc(%0) : memref<1x?xf64>
    "lo_spn.task"(%arg0, %1) ( {
    ^bb0(%arg2: index, %arg3: memref<?x4xi32>, %arg4: memref<1x?xf64>):  // no predecessors
      %4 = "lo_spn.batch_read"(%arg3, %arg2) {staticIndex = 3 : ui32} : (memref<?x4xi32>, index) -> i32
      %5 = "lo_spn.batch_read"(%arg3, %arg2) {staticIndex = 2 : ui32} : (memref<?x4xi32>, index) -> i32
      %6 = "lo_spn.batch_read"(%arg3, %arg2) {staticIndex = 0 : ui32} : (memref<?x4xi32>, index) -> i32
      %7 = "lo_spn.batch_read"(%arg3, %arg2) {staticIndex = 1 : ui32} : (memref<?x4xi32>, index) -> i32
      %8 = "lo_spn.body"(%4, %5, %6, %7) ( {
      ^bb0(%arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32):  // no predecessors
        // inputs to the SLP graph.
        %g1 = "lo_spn.gaussian"(%arg9) {mean = 1.100000e-01 : f64, stddev = 1.000000e+00 : f64, supportMarginal = false} : (i32) -> f64
        %g2 = "lo_spn.gaussian"(%arg10) {mean = 1.200000e-01 : f64, stddev = 7.500000e-01 : f64, supportMarginal = false} : (i32) -> f64
        %g3 = "lo_spn.gaussian"(%arg11) {mean = 1.300000e-01 : f64, stddev = 5.000000e-01 : f64, supportMarginal = false} : (i32) -> f64
        %g4 = "lo_spn.gaussian"(%arg12) {mean = 1.400000e-01 : f64, stddev = 2.500000e-01 : f64, supportMarginal = false} : (i32) -> f64
        %l1 = "lo_spn.constant"() {type = f64, value = 1.0 : f64} : () -> f64
        %l2 = "lo_spn.constant"() {type = f64, value = 2.0 : f64} : () -> f64
        %l3 = "lo_spn.constant"() {type = f64, value = 3.0 : f64} : () -> f64
        %l4 = "lo_spn.constant"() {type = f64, value = 4.0 : f64} : () -> f64
        %c1 = arith.constant 5.0 : f64
        %c2 = arith.constant 6.0 : f64
        %c3 = arith.constant 7.0 : f64
        %c4 = arith.constant 8.0 : f64
        // actual computation.
        %v1_1 = "lo_spn.mul"(%g1, %l1) : (f64, f64) -> f64
        %v1_2 = "lo_spn.mul"(%l2, %g2) : (f64, f64) -> f64
        %v1_3 = "lo_spn.mul"(%c3, %g3) : (f64, f64) -> f64
        %v1_4 = "lo_spn.mul"(%g4, %c4) : (f64, f64) -> f64
        %v2_1 = "lo_spn.mul"(%v1_1, %c1) : (f64, f64) -> f64
        %v2_2 = "lo_spn.mul"(%v1_2, %c2) : (f64, f64) -> f64
        %v2_3 = "lo_spn.mul"(%v1_3, %l3) : (f64, f64) -> f64
        %v2_4 = "lo_spn.mul"(%v1_4, %l4) : (f64, f64) -> f64
        %c5 = arith.constant 0.25 : f64
        %t1 = "lo_spn.mul"(%v2_1, %c5) : (f64, f64) -> f64
        %t2 = "lo_spn.mul"(%v2_2, %c5) : (f64, f64) -> f64
        %t3 = "lo_spn.mul"(%v2_3, %c5) : (f64, f64) -> f64
        %t4 = "lo_spn.mul"(%v2_4, %c5) : (f64, f64) -> f64
        %s1 = "lo_spn.add"(%t1, %t2) : (f64, f64) -> f64
        %s2 = "lo_spn.add"(%t3, %t4) : (f64, f64) -> f64
        %s3 = "lo_spn.add"(%s1, %s2) : (f64, f64) -> f64
        %r = "lo_spn.log"(%s3) : (f64) -> f64
        "lo_spn.yield"(%r) : (f64) -> ()
      }) : (i32, i32, i32, i32) -> f64
      "lo_spn.batch_write"(%arg4, %arg2, %8) {transposed = true} : (memref<1x?xf64>, index, f64) -> ()
      "lo_spn.return"() : () -> ()
    }) {batchSize = 1 : ui32} : (memref<?x4xi32>, memref<1x?xf64>) -> ()
    %2 = bufferization.to_tensor %1 : memref<1x?xf64>
    %3 = bufferization.to_memref %2 : memref<1x?xf64>
    "lo_spn.copy"(%3, %arg1) : (memref<1x?xf64>, memref<1x?xf64>) -> ()
    "lo_spn.return"() : () -> ()
  }) {sym_name = "spn_kernel", function_type = (memref<?x4xi32>, memref<1x?xf64>) -> ()} : () -> ()
}

// NOTE: Assertions have been autogenerated by utils/generate-test-checks.py


// CHECK-LABEL:   func.func @vec_task_0(
// CHECK-SAME:                     %[[VAL_0:.*]]: memref<?x4xi32>,
// CHECK-SAME:                     %[[VAL_1:.*]]: memref<1x?xf64>) {
// CHECK:           %[[VAL_2:.*]] = arith.constant 3 : i32
// CHECK:           %[[VAL_3:.*]] = arith.constant 2 : i32
// CHECK:           %[[VAL_4:.*]] = arith.constant 1 : i32
// CHECK:           %[[VAL_5:.*]] = arith.constant 0 : i32
// CHECK:           %[[VAL_6:.*]] = arith.constant dense<[2.500000e-01, 2.500000e-01, 2.500000e-01, 5.000000e+00]> : vector<4xf64>
// CHECK:           %[[VAL_7:.*]] = arith.constant dense<[4.000000e+00, 3.000000e+00, 6.000000e+00, 2.500000e-01]> : vector<4xf64>
// CHECK:           %[[VAL_8:.*]] = arith.constant dense<[8.000000e+00, 7.000000e+00, 2.000000e+00, 1.000000e+00]> : vector<4xf64>
// CHECK:           %[[VAL_9:.*]] = arith.constant dense<[0.62665706865775006, 1.2533141373155001, 1.8799712059732503, 2.5066282746310002]> : vector<4xf64>
// CHECK:           %[[VAL_10:.*]] = arith.constant dense<[-1.250000e-01, -5.000000e-01, -1.125000e+00, -2.000000e+00]> : vector<4xf64>
// CHECK:           %[[VAL_11:.*]] = arith.constant dense<[1.400000e-01, 1.300000e-01, 1.200000e-01, 1.100000e-01]> : vector<4xf64>
// CHECK:           %[[VAL_12:.*]] = arith.constant 0 : index
// CHECK:           %[[VAL_13:.*]] = arith.constant 0 : index
// CHECK:           %[[VAL_14:.*]] = vector.load %[[VAL_0]]{{\[}}%[[VAL_13]], %[[VAL_12]]] : memref<?x4xi32>, vector<4xi32>
// CHECK:           %[[VAL_15:.*]] = uitofp %[[VAL_14]] : vector<4xi32> to vector<4xf64>
// CHECK:           %[[VAL_16:.*]] = arith.subf %[[VAL_15]], %[[VAL_11]] : vector<4xf64>
// CHECK:           %[[VAL_17:.*]] = arith.mulf %[[VAL_16]], %[[VAL_16]] : vector<4xf64>
// CHECK:           %[[VAL_18:.*]] = divf %[[VAL_17]], %[[VAL_10]] : vector<4xf64>
// CHECK:           %[[VAL_19:.*]] = math.exp %[[VAL_18]] : vector<4xf64>
// CHECK:           %[[VAL_20:.*]] = divf %[[VAL_19]], %[[VAL_9]] : vector<4xf64>
// CHECK:           %[[VAL_21:.*]] = arith.mulf %[[VAL_20]], %[[VAL_8]] : vector<4xf64>
// CHECK:           %[[VAL_22:.*]] = arith.mulf %[[VAL_21]], %[[VAL_7]] : vector<4xf64>
// CHECK:           %[[VAL_23:.*]] = arith.mulf %[[VAL_22]], %[[VAL_6]] : vector<4xf64>
// CHECK:           %[[VAL_24:.*]] = vector.extractelement %[[VAL_23]]{{\[}}%[[VAL_2]] : i32] : vector<4xf64>
// CHECK:           %[[VAL_25:.*]] = vector.extractelement %[[VAL_23]]{{\[}}%[[VAL_3]] : i32] : vector<4xf64>
// CHECK:           %[[VAL_26:.*]] = vector.extractelement %[[VAL_23]]{{\[}}%[[VAL_4]] : i32] : vector<4xf64>
// CHECK:           %[[VAL_27:.*]] = vector.extractelement %[[VAL_23]]{{\[}}%[[VAL_5]] : i32] : vector<4xf64>
// CHECK:           %[[VAL_28:.*]] = "lo_spn.add"(%[[VAL_24]], %[[VAL_25]]) : (f64, f64) -> f64
// CHECK:           %[[VAL_29:.*]] = "lo_spn.add"(%[[VAL_26]], %[[VAL_27]]) : (f64, f64) -> f64
// CHECK:           %[[VAL_30:.*]] = "lo_spn.add"(%[[VAL_28]], %[[VAL_29]]) : (f64, f64) -> f64
// CHECK:           %[[VAL_31:.*]] = "lo_spn.log"(%[[VAL_30]]) : (f64) -> f64
// CHECK:           "lo_spn.batch_write"(%[[VAL_1]], %[[VAL_13]], %[[VAL_31]]) {transposed = true} : (memref<1x?xf64>, index, f64) -> ()
// CHECK:           "lo_spn.return"() : () -> ()
// CHECK:         }

// CHECK-LABEL:   func.func @spn_kernel(
// CHECK-SAME:                     %[[VAL_0:.*]]: memref<?x4xi32>,
// CHECK-SAME:                     %[[VAL_1:.*]]: memref<1x?xf64>) {
// CHECK:           %[[VAL_2:.*]] = arith.constant 0 : index
// CHECK:           %[[VAL_3:.*]] = memref.dim %[[VAL_0]], %[[VAL_2]] : memref<?x4xi32>
// CHECK:           %[[VAL_4:.*]] = memref.alloc(%[[VAL_3]]) : memref<1x?xf64>
// CHECK:           call @vec_task_0(%[[VAL_0]], %[[VAL_4]]) : (memref<?x4xi32>, memref<1x?xf64>) -> ()
// CHECK:           %[[VAL_5:.*]] = bufferization.to_tensor %[[VAL_4]] : memref<1x?xf64>
// CHECK:           %[[VAL_6:.*]] = bufferization.to_memref %[[VAL_5]] : memref<1x?xf64>
// CHECK:           "lo_spn.copy"(%[[VAL_6]], %[[VAL_1]]) : (memref<1x?xf64>, memref<1x?xf64>) -> ()
// CHECK:           "lo_spn.return"() : () -> ()
// CHECK:         }

