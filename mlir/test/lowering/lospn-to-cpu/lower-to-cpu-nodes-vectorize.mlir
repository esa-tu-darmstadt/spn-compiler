// RUN: %optcall --vectorize-lospn-nodes %s | FileCheck %s
// Test (re)generated by regenerate_tests.py.
// NOTE: Assertions have been autogenerated by utils/generate-test-checks.py

// The script is designed to make adding checks to
// a test case fast, it is *not* designed to be authoritative
// about what constitutes a good test! The CHECK should be
// minimized and named to reflect the test intent.

// CHECK-LABEL:   memref.global "private" constant @histogram_vec_1 : memref<2xf64> = dense<[4.500000e-01, 5.500000e-01]>
// CHECK:         memref.global "private" constant @histogram_vec_0 : memref<2xf64> = dense<[2.500000e-01, 7.500000e-01]>
// CHECK:         memref.global "private" constant @categorical_vec_1 : memref<3xf64> = dense<[2.500000e-01, 6.250000e-01, 1.250000e-01]>
// CHECK:         memref.global "private" constant @categorical_vec_0 : memref<3xf64> = dense<[3.500000e-01, 5.500000e-01, 1.000000e-01]>
// RUN: %optcall --vectorize-lospn-nodes %s | FileCheck %s
module {
// CHECK-LABEL:   func.func @vec_task_0(
// CHECK-SAME:                          %[[VAL_0:.*]]: memref<?x6xf64>,
// CHECK-SAME:                          %[[VAL_1:.*]]: memref<1x?xf64>) {
// CHECK:           %[[VAL_2:.*]] = arith.constant 0 : index
// CHECK:           %[[VAL_3:.*]] = memref.dim %[[VAL_0]], %[[VAL_2]] : memref<?x6xf64>
// CHECK:           %[[VAL_4:.*]] = arith.constant 4 : index
// CHECK:           %[[VAL_5:.*]] = arith.remui %[[VAL_3]], %[[VAL_4]] : index
// CHECK:           %[[VAL_6:.*]] = arith.subi %[[VAL_3]], %[[VAL_5]] : index
// CHECK:           %[[VAL_7:.*]] = arith.constant 0 : index
// CHECK:           %[[VAL_8:.*]] = arith.constant 4 : index
// CHECK:           scf.for %[[VAL_9:.*]] = %[[VAL_7]] to %[[VAL_6]] step %[[VAL_8]] {
// CHECK:             %[[VAL_10:.*]] = arith.index_cast %[[VAL_9]] : index to i64
// CHECK:             %[[VAL_11:.*]] = vector.broadcast %[[VAL_10]] : i64 to vector<4xi64>
// CHECK:             %[[VAL_12:.*]] = arith.constant dense<[0, 6, 12, 18]> : vector<4xi64>
// CHECK:             %[[VAL_13:.*]] = arith.constant 6 : i64
// CHECK:             %[[VAL_14:.*]] = vector.broadcast %[[VAL_13]] : i64 to vector<4xi64>
// CHECK:             %[[VAL_15:.*]] = arith.muli %[[VAL_11]], %[[VAL_14]] : vector<4xi64>
// CHECK:             %[[VAL_16:.*]] = arith.addi %[[VAL_15]], %[[VAL_12]] : vector<4xi64>
// CHECK:             %[[VAL_17:.*]] = arith.constant 0.000000e+00 : f64
// CHECK:             %[[VAL_18:.*]] = vector.broadcast %[[VAL_17]] : f64 to vector<4xf64>
// CHECK:             %[[VAL_19:.*]] = arith.constant true
// CHECK:             %[[VAL_20:.*]] = vector.broadcast %[[VAL_19]] : i1 to vector<4xi1>
// CHECK:             %[[VAL_21:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_22:.*]] = memref.dim %[[VAL_0]], %[[VAL_21]] : memref<?x6xf64>
// CHECK:             %[[VAL_23:.*]] = arith.constant 6 : index
// CHECK:             %[[VAL_24:.*]] = arith.muli %[[VAL_22]], %[[VAL_23]] : index
// CHECK:             %[[VAL_25:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: {{\[}}%[[VAL_24]]], strides: [1] : memref<?x6xf64> to memref<?xf64>
// CHECK:             %[[VAL_26:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_27:.*]] = vector.gather %[[VAL_25]]{{\[}}%[[VAL_26]]] {{\[}}%[[VAL_16]]], %[[VAL_20]], %[[VAL_18]] : memref<?xf64>, vector<4xi64>, vector<4xi1>, vector<4xf64> into vector<4xf64>
// CHECK:             %[[VAL_28:.*]] = arith.index_cast %[[VAL_9]] : index to i64
// CHECK:             %[[VAL_29:.*]] = vector.broadcast %[[VAL_28]] : i64 to vector<4xi64>
// CHECK:             %[[VAL_30:.*]] = arith.constant dense<[1, 7, 13, 19]> : vector<4xi64>
// CHECK:             %[[VAL_31:.*]] = arith.constant 6 : i64
// CHECK:             %[[VAL_32:.*]] = vector.broadcast %[[VAL_31]] : i64 to vector<4xi64>
// CHECK:             %[[VAL_33:.*]] = arith.muli %[[VAL_29]], %[[VAL_32]] : vector<4xi64>
// CHECK:             %[[VAL_34:.*]] = arith.addi %[[VAL_33]], %[[VAL_30]] : vector<4xi64>
// CHECK:             %[[VAL_35:.*]] = arith.constant 0.000000e+00 : f64
// CHECK:             %[[VAL_36:.*]] = vector.broadcast %[[VAL_35]] : f64 to vector<4xf64>
// CHECK:             %[[VAL_37:.*]] = arith.constant true
// CHECK:             %[[VAL_38:.*]] = vector.broadcast %[[VAL_37]] : i1 to vector<4xi1>
// CHECK:             %[[VAL_39:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_40:.*]] = memref.dim %[[VAL_0]], %[[VAL_39]] : memref<?x6xf64>
// CHECK:             %[[VAL_41:.*]] = arith.constant 6 : index
// CHECK:             %[[VAL_42:.*]] = arith.muli %[[VAL_40]], %[[VAL_41]] : index
// CHECK:             %[[VAL_43:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: {{\[}}%[[VAL_42]]], strides: [1] : memref<?x6xf64> to memref<?xf64>
// CHECK:             %[[VAL_44:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_45:.*]] = vector.gather %[[VAL_43]]{{\[}}%[[VAL_44]]] {{\[}}%[[VAL_34]]], %[[VAL_38]], %[[VAL_36]] : memref<?xf64>, vector<4xi64>, vector<4xi1>, vector<4xf64> into vector<4xf64>
// CHECK:             %[[VAL_46:.*]] = arith.index_cast %[[VAL_9]] : index to i64
// CHECK:             %[[VAL_47:.*]] = vector.broadcast %[[VAL_46]] : i64 to vector<4xi64>
// CHECK:             %[[VAL_48:.*]] = arith.constant dense<[2, 8, 14, 20]> : vector<4xi64>
// CHECK:             %[[VAL_49:.*]] = arith.constant 6 : i64
// CHECK:             %[[VAL_50:.*]] = vector.broadcast %[[VAL_49]] : i64 to vector<4xi64>
// CHECK:             %[[VAL_51:.*]] = arith.muli %[[VAL_47]], %[[VAL_50]] : vector<4xi64>
// CHECK:             %[[VAL_52:.*]] = arith.addi %[[VAL_51]], %[[VAL_48]] : vector<4xi64>
// CHECK:             %[[VAL_53:.*]] = arith.constant 0.000000e+00 : f64
// CHECK:             %[[VAL_54:.*]] = vector.broadcast %[[VAL_53]] : f64 to vector<4xf64>
// CHECK:             %[[VAL_55:.*]] = arith.constant true
// CHECK:             %[[VAL_56:.*]] = vector.broadcast %[[VAL_55]] : i1 to vector<4xi1>
// CHECK:             %[[VAL_57:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_58:.*]] = memref.dim %[[VAL_0]], %[[VAL_57]] : memref<?x6xf64>
// CHECK:             %[[VAL_59:.*]] = arith.constant 6 : index
// CHECK:             %[[VAL_60:.*]] = arith.muli %[[VAL_58]], %[[VAL_59]] : index
// CHECK:             %[[VAL_61:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: {{\[}}%[[VAL_60]]], strides: [1] : memref<?x6xf64> to memref<?xf64>
// CHECK:             %[[VAL_62:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_63:.*]] = vector.gather %[[VAL_61]]{{\[}}%[[VAL_62]]] {{\[}}%[[VAL_52]]], %[[VAL_56]], %[[VAL_54]] : memref<?xf64>, vector<4xi64>, vector<4xi1>, vector<4xf64> into vector<4xf64>
// CHECK:             %[[VAL_64:.*]] = arith.index_cast %[[VAL_9]] : index to i64
// CHECK:             %[[VAL_65:.*]] = vector.broadcast %[[VAL_64]] : i64 to vector<4xi64>
// CHECK:             %[[VAL_66:.*]] = arith.constant dense<[3, 9, 15, 21]> : vector<4xi64>
// CHECK:             %[[VAL_67:.*]] = arith.constant 6 : i64
// CHECK:             %[[VAL_68:.*]] = vector.broadcast %[[VAL_67]] : i64 to vector<4xi64>
// CHECK:             %[[VAL_69:.*]] = arith.muli %[[VAL_65]], %[[VAL_68]] : vector<4xi64>
// CHECK:             %[[VAL_70:.*]] = arith.addi %[[VAL_69]], %[[VAL_66]] : vector<4xi64>
// CHECK:             %[[VAL_71:.*]] = arith.constant 0.000000e+00 : f64
// CHECK:             %[[VAL_72:.*]] = vector.broadcast %[[VAL_71]] : f64 to vector<4xf64>
// CHECK:             %[[VAL_73:.*]] = arith.constant true
// CHECK:             %[[VAL_74:.*]] = vector.broadcast %[[VAL_73]] : i1 to vector<4xi1>
// CHECK:             %[[VAL_75:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_76:.*]] = memref.dim %[[VAL_0]], %[[VAL_75]] : memref<?x6xf64>
// CHECK:             %[[VAL_77:.*]] = arith.constant 6 : index
// CHECK:             %[[VAL_78:.*]] = arith.muli %[[VAL_76]], %[[VAL_77]] : index
// CHECK:             %[[VAL_79:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: {{\[}}%[[VAL_78]]], strides: [1] : memref<?x6xf64> to memref<?xf64>
// CHECK:             %[[VAL_80:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_81:.*]] = vector.gather %[[VAL_79]]{{\[}}%[[VAL_80]]] {{\[}}%[[VAL_70]]], %[[VAL_74]], %[[VAL_72]] : memref<?xf64>, vector<4xi64>, vector<4xi1>, vector<4xf64> into vector<4xf64>
// CHECK:             %[[VAL_82:.*]] = arith.index_cast %[[VAL_9]] : index to i64
// CHECK:             %[[VAL_83:.*]] = vector.broadcast %[[VAL_82]] : i64 to vector<4xi64>
// CHECK:             %[[VAL_84:.*]] = arith.constant dense<[4, 10, 16, 22]> : vector<4xi64>
// CHECK:             %[[VAL_85:.*]] = arith.constant 6 : i64
// CHECK:             %[[VAL_86:.*]] = vector.broadcast %[[VAL_85]] : i64 to vector<4xi64>
// CHECK:             %[[VAL_87:.*]] = arith.muli %[[VAL_83]], %[[VAL_86]] : vector<4xi64>
// CHECK:             %[[VAL_88:.*]] = arith.addi %[[VAL_87]], %[[VAL_84]] : vector<4xi64>
// CHECK:             %[[VAL_89:.*]] = arith.constant 0.000000e+00 : f64
// CHECK:             %[[VAL_90:.*]] = vector.broadcast %[[VAL_89]] : f64 to vector<4xf64>
// CHECK:             %[[VAL_91:.*]] = arith.constant true
// CHECK:             %[[VAL_92:.*]] = vector.broadcast %[[VAL_91]] : i1 to vector<4xi1>
// CHECK:             %[[VAL_93:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_94:.*]] = memref.dim %[[VAL_0]], %[[VAL_93]] : memref<?x6xf64>
// CHECK:             %[[VAL_95:.*]] = arith.constant 6 : index
// CHECK:             %[[VAL_96:.*]] = arith.muli %[[VAL_94]], %[[VAL_95]] : index
// CHECK:             %[[VAL_97:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: {{\[}}%[[VAL_96]]], strides: [1] : memref<?x6xf64> to memref<?xf64>
// CHECK:             %[[VAL_98:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_99:.*]] = vector.gather %[[VAL_97]]{{\[}}%[[VAL_98]]] {{\[}}%[[VAL_88]]], %[[VAL_92]], %[[VAL_90]] : memref<?xf64>, vector<4xi64>, vector<4xi1>, vector<4xf64> into vector<4xf64>
// CHECK:             %[[VAL_100:.*]] = arith.index_cast %[[VAL_9]] : index to i64
// CHECK:             %[[VAL_101:.*]] = vector.broadcast %[[VAL_100]] : i64 to vector<4xi64>
// CHECK:             %[[VAL_102:.*]] = arith.constant dense<[5, 11, 17, 23]> : vector<4xi64>
// CHECK:             %[[VAL_103:.*]] = arith.constant 6 : i64
// CHECK:             %[[VAL_104:.*]] = vector.broadcast %[[VAL_103]] : i64 to vector<4xi64>
// CHECK:             %[[VAL_105:.*]] = arith.muli %[[VAL_101]], %[[VAL_104]] : vector<4xi64>
// CHECK:             %[[VAL_106:.*]] = arith.addi %[[VAL_105]], %[[VAL_102]] : vector<4xi64>
// CHECK:             %[[VAL_107:.*]] = arith.constant 0.000000e+00 : f64
// CHECK:             %[[VAL_108:.*]] = vector.broadcast %[[VAL_107]] : f64 to vector<4xf64>
// CHECK:             %[[VAL_109:.*]] = arith.constant true
// CHECK:             %[[VAL_110:.*]] = vector.broadcast %[[VAL_109]] : i1 to vector<4xi1>
// CHECK:             %[[VAL_111:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_112:.*]] = memref.dim %[[VAL_0]], %[[VAL_111]] : memref<?x6xf64>
// CHECK:             %[[VAL_113:.*]] = arith.constant 6 : index
// CHECK:             %[[VAL_114:.*]] = arith.muli %[[VAL_112]], %[[VAL_113]] : index
// CHECK:             %[[VAL_115:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: {{\[}}%[[VAL_114]]], strides: [1] : memref<?x6xf64> to memref<?xf64>
// CHECK:             %[[VAL_116:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_117:.*]] = vector.gather %[[VAL_115]]{{\[}}%[[VAL_116]]] {{\[}}%[[VAL_106]]], %[[VAL_110]], %[[VAL_108]] : memref<?xf64>, vector<4xi64>, vector<4xi1>, vector<4xf64> into vector<4xf64>
// CHECK:             %[[VAL_118:.*]] = memref.get_global @categorical_vec_0 : memref<3xf64>
// CHECK:             %[[VAL_119:.*]] = arith.fptoui %[[VAL_27]] : vector<4xf64> to vector<4xi64>
// CHECK:             %[[VAL_120:.*]] = arith.constant 0.000000e+00 : f64
// CHECK:             %[[VAL_121:.*]] = vector.broadcast %[[VAL_120]] : f64 to vector<4xf64>
// CHECK:             %[[VAL_122:.*]] = arith.constant true
// CHECK:             %[[VAL_123:.*]] = vector.broadcast %[[VAL_122]] : i1 to vector<4xi1>
// CHECK:             %[[VAL_124:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_125:.*]] = vector.gather %[[VAL_118]]{{\[}}%[[VAL_124]]] {{\[}}%[[VAL_119]]], %[[VAL_123]], %[[VAL_121]] : memref<3xf64>, vector<4xi64>, vector<4xi1>, vector<4xf64> into vector<4xf64>
// CHECK:             %[[VAL_126:.*]] = memref.get_global @categorical_vec_1 : memref<3xf64>
// CHECK:             %[[VAL_127:.*]] = arith.fptoui %[[VAL_45]] : vector<4xf64> to vector<4xi64>
// CHECK:             %[[VAL_128:.*]] = arith.constant 0.000000e+00 : f64
// CHECK:             %[[VAL_129:.*]] = vector.broadcast %[[VAL_128]] : f64 to vector<4xf64>
// CHECK:             %[[VAL_130:.*]] = arith.constant true
// CHECK:             %[[VAL_131:.*]] = vector.broadcast %[[VAL_130]] : i1 to vector<4xi1>
// CHECK:             %[[VAL_132:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_133:.*]] = vector.gather %[[VAL_126]]{{\[}}%[[VAL_132]]] {{\[}}%[[VAL_127]]], %[[VAL_131]], %[[VAL_129]] : memref<3xf64>, vector<4xi64>, vector<4xi1>, vector<4xf64> into vector<4xf64>
// CHECK:             %[[VAL_134:.*]] = memref.get_global @histogram_vec_0 : memref<2xf64>
// CHECK:             %[[VAL_135:.*]] = arith.fptoui %[[VAL_63]] : vector<4xf64> to vector<4xi64>
// CHECK:             %[[VAL_136:.*]] = arith.constant 0.000000e+00 : f64
// CHECK:             %[[VAL_137:.*]] = vector.broadcast %[[VAL_136]] : f64 to vector<4xf64>
// CHECK:             %[[VAL_138:.*]] = arith.constant true
// CHECK:             %[[VAL_139:.*]] = vector.broadcast %[[VAL_138]] : i1 to vector<4xi1>
// CHECK:             %[[VAL_140:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_141:.*]] = vector.gather %[[VAL_134]]{{\[}}%[[VAL_140]]] {{\[}}%[[VAL_135]]], %[[VAL_139]], %[[VAL_137]] : memref<2xf64>, vector<4xi64>, vector<4xi1>, vector<4xf64> into vector<4xf64>
// CHECK:             %[[VAL_142:.*]] = memref.get_global @histogram_vec_1 : memref<2xf64>
// CHECK:             %[[VAL_143:.*]] = arith.fptoui %[[VAL_81]] : vector<4xf64> to vector<4xi64>
// CHECK:             %[[VAL_144:.*]] = arith.constant 0.000000e+00 : f64
// CHECK:             %[[VAL_145:.*]] = vector.broadcast %[[VAL_144]] : f64 to vector<4xf64>
// CHECK:             %[[VAL_146:.*]] = arith.constant true
// CHECK:             %[[VAL_147:.*]] = vector.broadcast %[[VAL_146]] : i1 to vector<4xi1>
// CHECK:             %[[VAL_148:.*]] = arith.constant 0 : index
// CHECK:             %[[VAL_149:.*]] = vector.gather %[[VAL_142]]{{\[}}%[[VAL_148]]] {{\[}}%[[VAL_143]]], %[[VAL_147]], %[[VAL_145]] : memref<2xf64>, vector<4xi64>, vector<4xi1>, vector<4xf64> into vector<4xf64>
// CHECK:             %[[VAL_150:.*]] = arith.constant 0.3989422804014327 : f64
// CHECK:             %[[VAL_151:.*]] = vector.broadcast %[[VAL_150]] : f64 to vector<4xf64>
// CHECK:             %[[VAL_152:.*]] = arith.constant -5.000000e-01 : f64
// CHECK:             %[[VAL_153:.*]] = vector.broadcast %[[VAL_152]] : f64 to vector<4xf64>
// CHECK:             %[[VAL_154:.*]] = arith.constant 5.000000e-01 : f64
// CHECK:             %[[VAL_155:.*]] = vector.broadcast %[[VAL_154]] : f64 to vector<4xf64>
// CHECK:             %[[VAL_156:.*]] = arith.subf %[[VAL_99]], %[[VAL_155]] : vector<4xf64>
// CHECK:             %[[VAL_157:.*]] = arith.mulf %[[VAL_156]], %[[VAL_156]] : vector<4xf64>
// CHECK:             %[[VAL_158:.*]] = arith.mulf %[[VAL_157]], %[[VAL_153]] : vector<4xf64>
// CHECK:             %[[VAL_159:.*]] = math.exp %[[VAL_158]] : vector<4xf64>
// CHECK:             %[[VAL_160:.*]] = arith.mulf %[[VAL_151]], %[[VAL_159]] : vector<4xf64>
// CHECK:             %[[VAL_161:.*]] = arith.constant 3.9894228040143269 : f64
// CHECK:             %[[VAL_162:.*]] = vector.broadcast %[[VAL_161]] : f64 to vector<4xf64>
// CHECK:             %[[VAL_163:.*]] = arith.constant -49.999999999999993 : f64
// CHECK:             %[[VAL_164:.*]] = vector.broadcast %[[VAL_163]] : f64 to vector<4xf64>
// CHECK:             %[[VAL_165:.*]] = arith.constant 2.500000e-01 : f64
// CHECK:             %[[VAL_166:.*]] = vector.broadcast %[[VAL_165]] : f64 to vector<4xf64>
// CHECK:             %[[VAL_167:.*]] = arith.subf %[[VAL_117]], %[[VAL_166]] : vector<4xf64>
// CHECK:             %[[VAL_168:.*]] = arith.mulf %[[VAL_167]], %[[VAL_167]] : vector<4xf64>
// CHECK:             %[[VAL_169:.*]] = arith.mulf %[[VAL_168]], %[[VAL_164]] : vector<4xf64>
// CHECK:             %[[VAL_170:.*]] = math.exp %[[VAL_169]] : vector<4xf64>
// CHECK:             %[[VAL_171:.*]] = arith.mulf %[[VAL_162]], %[[VAL_170]] : vector<4xf64>
// CHECK:             %[[VAL_172:.*]] = arith.mulf %[[VAL_125]], %[[VAL_133]] : vector<4xf64>
// CHECK:             %[[VAL_173:.*]] = arith.mulf %[[VAL_172]], %[[VAL_141]] : vector<4xf64>
// CHECK:             %[[VAL_174:.*]] = arith.constant 1.000000e-01 : f64
// CHECK:             %[[VAL_175:.*]] = vector.broadcast %[[VAL_174]] : f64 to vector<4xf64>
// CHECK:             %[[VAL_176:.*]] = arith.mulf %[[VAL_173]], %[[VAL_175]] : vector<4xf64>
// CHECK:             %[[VAL_177:.*]] = arith.mulf %[[VAL_149]], %[[VAL_160]] : vector<4xf64>
// CHECK:             %[[VAL_178:.*]] = arith.mulf %[[VAL_177]], %[[VAL_171]] : vector<4xf64>
// CHECK:             %[[VAL_179:.*]] = arith.constant 1.000000e-01 : f64
// CHECK:             %[[VAL_180:.*]] = vector.broadcast %[[VAL_179]] : f64 to vector<4xf64>
// CHECK:             %[[VAL_181:.*]] = arith.mulf %[[VAL_178]], %[[VAL_180]] : vector<4xf64>
// CHECK:             %[[VAL_182:.*]] = arith.addf %[[VAL_176]], %[[VAL_181]] : vector<4xf64>
// CHECK:             %[[VAL_183:.*]] = math.log %[[VAL_182]] : vector<4xf64>
// CHECK:             %[[VAL_184:.*]] = arith.constant 0 : index
// CHECK:             vector.transfer_write %[[VAL_183]], %[[VAL_1]]{{\[}}%[[VAL_184]], %[[VAL_9]]] : vector<4xf64>, memref<1x?xf64>
// CHECK:           }
// CHECK:           %[[VAL_185:.*]] = arith.constant 1 : index
// CHECK:           scf.for %[[VAL_186:.*]] = %[[VAL_6]] to %[[VAL_3]] step %[[VAL_185]] {
// CHECK:             %[[VAL_187:.*]] = "lo_spn.batch_read"(%[[VAL_0]], %[[VAL_186]]) <{staticIndex = 0 : ui32}> : (memref<?x6xf64>, index) -> f64
// CHECK:             %[[VAL_188:.*]] = "lo_spn.batch_read"(%[[VAL_0]], %[[VAL_186]]) <{staticIndex = 1 : ui32}> : (memref<?x6xf64>, index) -> f64
// CHECK:             %[[VAL_189:.*]] = "lo_spn.batch_read"(%[[VAL_0]], %[[VAL_186]]) <{staticIndex = 2 : ui32}> : (memref<?x6xf64>, index) -> f64
// CHECK:             %[[VAL_190:.*]] = "lo_spn.batch_read"(%[[VAL_0]], %[[VAL_186]]) <{staticIndex = 3 : ui32}> : (memref<?x6xf64>, index) -> f64
// CHECK:             %[[VAL_191:.*]] = "lo_spn.batch_read"(%[[VAL_0]], %[[VAL_186]]) <{staticIndex = 4 : ui32}> : (memref<?x6xf64>, index) -> f64
// CHECK:             %[[VAL_192:.*]] = "lo_spn.batch_read"(%[[VAL_0]], %[[VAL_186]]) <{staticIndex = 5 : ui32}> : (memref<?x6xf64>, index) -> f64
// CHECK:             %[[VAL_193:.*]] = "lo_spn.categorical"(%[[VAL_187]]) <{probabilities = [3.500000e-01, 5.500000e-01, 1.000000e-01], supportMarginal = false}> : (f64) -> f64
// CHECK:             %[[VAL_194:.*]] = "lo_spn.categorical"(%[[VAL_188]]) <{probabilities = [2.500000e-01, 6.250000e-01, 1.250000e-01], supportMarginal = false}> : (f64) -> f64
// CHECK:             %[[VAL_195:.*]] = "lo_spn.histogram"(%[[VAL_189]]) <{bucketCount = 2 : ui32, buckets = [#hi_spn.bucket<0 to 1 = 2.500000e-01>, #hi_spn.bucket<1 to 2 = 7.500000e-01>], supportMarginal = false}> : (f64) -> f64
// CHECK:             %[[VAL_196:.*]] = "lo_spn.histogram"(%[[VAL_190]]) <{bucketCount = 2 : ui32, buckets = [#hi_spn.bucket<0 to 1 = 4.500000e-01>, #hi_spn.bucket<1 to 2 = 5.500000e-01>], supportMarginal = false}> : (f64) -> f64
// CHECK:             %[[VAL_197:.*]] = "lo_spn.gaussian"(%[[VAL_191]]) <{mean = 5.000000e-01 : f64, stddev = 1.000000e+00 : f64, supportMarginal = false}> : (f64) -> f64
// CHECK:             %[[VAL_198:.*]] = "lo_spn.gaussian"(%[[VAL_192]]) <{mean = 2.500000e-01 : f64, stddev = 1.000000e-01 : f64, supportMarginal = false}> : (f64) -> f64
// CHECK:             %[[VAL_199:.*]] = "lo_spn.mul"(%[[VAL_193]], %[[VAL_194]]) : (f64, f64) -> f64
// CHECK:             %[[VAL_200:.*]] = "lo_spn.mul"(%[[VAL_199]], %[[VAL_195]]) : (f64, f64) -> f64
// CHECK:             %[[VAL_201:.*]] = "lo_spn.constant"() <{value = 1.000000e-01 : f64}> : () -> f64
// CHECK:             %[[VAL_202:.*]] = "lo_spn.mul"(%[[VAL_200]], %[[VAL_201]]) : (f64, f64) -> f64
// CHECK:             %[[VAL_203:.*]] = "lo_spn.mul"(%[[VAL_196]], %[[VAL_197]]) : (f64, f64) -> f64
// CHECK:             %[[VAL_204:.*]] = "lo_spn.mul"(%[[VAL_203]], %[[VAL_198]]) : (f64, f64) -> f64
// CHECK:             %[[VAL_205:.*]] = "lo_spn.constant"() <{value = 1.000000e-01 : f64}> : () -> f64
// CHECK:             %[[VAL_206:.*]] = "lo_spn.mul"(%[[VAL_204]], %[[VAL_205]]) : (f64, f64) -> f64
// CHECK:             %[[VAL_207:.*]] = "lo_spn.add"(%[[VAL_202]], %[[VAL_206]]) : (f64, f64) -> f64
// CHECK:             %[[VAL_208:.*]] = "lo_spn.log"(%[[VAL_207]]) : (f64) -> f64
// CHECK:             "lo_spn.batch_write"(%[[VAL_1]], %[[VAL_186]], %[[VAL_208]]) <{transposed = true}> : (memref<1x?xf64>, index, f64) -> ()
// CHECK:           }
// CHECK:           return
// CHECK:         }
  func.func @vec_task_0(%arg0: memref<?x6xf64>, %arg1: memref<1x?xf64>) {
    %c0 = arith.constant 0 : index
    %dim = memref.dim %arg0, %c0 : memref<?x6xf64>
    %c4 = arith.constant 4 : index
    %0 = arith.remui %dim, %c4 : index
    %1 = arith.subi %dim, %0 : index
    %c0_0 = arith.constant 0 : index
    %c4_1 = arith.constant 4 : index
    scf.for %arg2 = %c0_0 to %1 step %c4_1 {
      %2 = "lo_spn.batch_read"(%arg0, %arg2) <{staticIndex = 0 : ui32}> {vector_width = 4 : i32} : (memref<?x6xf64>, index) -> f64
      %3 = "lo_spn.batch_read"(%arg0, %arg2) <{staticIndex = 1 : ui32}> {vector_width = 4 : i32} : (memref<?x6xf64>, index) -> f64
      %4 = "lo_spn.batch_read"(%arg0, %arg2) <{staticIndex = 2 : ui32}> {vector_width = 4 : i32} : (memref<?x6xf64>, index) -> f64
      %5 = "lo_spn.batch_read"(%arg0, %arg2) <{staticIndex = 3 : ui32}> {vector_width = 4 : i32} : (memref<?x6xf64>, index) -> f64
      %6 = "lo_spn.batch_read"(%arg0, %arg2) <{staticIndex = 4 : ui32}> {vector_width = 4 : i32} : (memref<?x6xf64>, index) -> f64
      %7 = "lo_spn.batch_read"(%arg0, %arg2) <{staticIndex = 5 : ui32}> {vector_width = 4 : i32} : (memref<?x6xf64>, index) -> f64
      %8 = "lo_spn.categorical"(%2) <{probabilities = [3.500000e-01, 5.500000e-01, 1.000000e-01], supportMarginal = false}> {vector_width = 4 : i32} : (f64) -> f64
      %9 = "lo_spn.categorical"(%3) <{probabilities = [2.500000e-01, 6.250000e-01, 1.250000e-01], supportMarginal = false}> {vector_width = 4 : i32} : (f64) -> f64
      %10 = "lo_spn.histogram"(%4) <{bucketCount = 2 : ui32, buckets = [#hi_spn.bucket<0 to 1 = 2.500000e-01>, #hi_spn.bucket<1 to 2 = 7.500000e-01>], supportMarginal = false}> {vector_width = 4 : i32} : (f64) -> f64
      %11 = "lo_spn.histogram"(%5) <{bucketCount = 2 : ui32, buckets = [#hi_spn.bucket<0 to 1 = 4.500000e-01>, #hi_spn.bucket<1 to 2 = 5.500000e-01>], supportMarginal = false}> {vector_width = 4 : i32} : (f64) -> f64
      %12 = "lo_spn.gaussian"(%6) <{mean = 5.000000e-01 : f64, stddev = 1.000000e+00 : f64, supportMarginal = false}> {vector_width = 4 : i32} : (f64) -> f64
      %13 = "lo_spn.gaussian"(%7) <{mean = 2.500000e-01 : f64, stddev = 1.000000e-01 : f64, supportMarginal = false}> {vector_width = 4 : i32} : (f64) -> f64
      %14 = "lo_spn.mul"(%8, %9) {vector_width = 4 : i32} : (f64, f64) -> f64
      %15 = "lo_spn.mul"(%14, %10) {vector_width = 4 : i32} : (f64, f64) -> f64
      %16 = "lo_spn.constant"() <{value = 1.000000e-01 : f64}> {vector_width = 4 : i32} : () -> f64
      %17 = "lo_spn.mul"(%15, %16) {vector_width = 4 : i32} : (f64, f64) -> f64
      %18 = "lo_spn.mul"(%11, %12) {vector_width = 4 : i32} : (f64, f64) -> f64
      %19 = "lo_spn.mul"(%18, %13) {vector_width = 4 : i32} : (f64, f64) -> f64
      %20 = "lo_spn.constant"() <{value = 1.000000e-01 : f64}> {vector_width = 4 : i32} : () -> f64
      %21 = "lo_spn.mul"(%19, %20) {vector_width = 4 : i32} : (f64, f64) -> f64
      %22 = "lo_spn.add"(%17, %21) {vector_width = 4 : i32} : (f64, f64) -> f64
      %23 = "lo_spn.log"(%22) {vector_width = 4 : i32} : (f64) -> f64
      "lo_spn.batch_write"(%arg1, %arg2, %23) <{transposed = true}> {vector_width = 4 : i32} : (memref<1x?xf64>, index, f64) -> ()
    }
    %c1 = arith.constant 1 : index
    scf.for %arg2 = %1 to %dim step %c1 {
      %2 = "lo_spn.batch_read"(%arg0, %arg2) <{staticIndex = 0 : ui32}> : (memref<?x6xf64>, index) -> f64
      %3 = "lo_spn.batch_read"(%arg0, %arg2) <{staticIndex = 1 : ui32}> : (memref<?x6xf64>, index) -> f64
      %4 = "lo_spn.batch_read"(%arg0, %arg2) <{staticIndex = 2 : ui32}> : (memref<?x6xf64>, index) -> f64
      %5 = "lo_spn.batch_read"(%arg0, %arg2) <{staticIndex = 3 : ui32}> : (memref<?x6xf64>, index) -> f64
      %6 = "lo_spn.batch_read"(%arg0, %arg2) <{staticIndex = 4 : ui32}> : (memref<?x6xf64>, index) -> f64
      %7 = "lo_spn.batch_read"(%arg0, %arg2) <{staticIndex = 5 : ui32}> : (memref<?x6xf64>, index) -> f64
      %8 = "lo_spn.categorical"(%2) <{probabilities = [3.500000e-01, 5.500000e-01, 1.000000e-01], supportMarginal = false}> : (f64) -> f64
      %9 = "lo_spn.categorical"(%3) <{probabilities = [2.500000e-01, 6.250000e-01, 1.250000e-01], supportMarginal = false}> : (f64) -> f64
      %10 = "lo_spn.histogram"(%4) <{bucketCount = 2 : ui32, buckets = [#hi_spn.bucket<0 to 1 = 2.500000e-01>, #hi_spn.bucket<1 to 2 = 7.500000e-01>], supportMarginal = false}> : (f64) -> f64
      %11 = "lo_spn.histogram"(%5) <{bucketCount = 2 : ui32, buckets = [#hi_spn.bucket<0 to 1 = 4.500000e-01>, #hi_spn.bucket<1 to 2 = 5.500000e-01>], supportMarginal = false}> : (f64) -> f64
      %12 = "lo_spn.gaussian"(%6) <{mean = 5.000000e-01 : f64, stddev = 1.000000e+00 : f64, supportMarginal = false}> : (f64) -> f64
      %13 = "lo_spn.gaussian"(%7) <{mean = 2.500000e-01 : f64, stddev = 1.000000e-01 : f64, supportMarginal = false}> : (f64) -> f64
      %14 = "lo_spn.mul"(%8, %9) : (f64, f64) -> f64
      %15 = "lo_spn.mul"(%14, %10) : (f64, f64) -> f64
      %16 = "lo_spn.constant"() <{value = 1.000000e-01 : f64}> : () -> f64
      %17 = "lo_spn.mul"(%15, %16) : (f64, f64) -> f64
      %18 = "lo_spn.mul"(%11, %12) : (f64, f64) -> f64
      %19 = "lo_spn.mul"(%18, %13) : (f64, f64) -> f64
      %20 = "lo_spn.constant"() <{value = 1.000000e-01 : f64}> : () -> f64
      %21 = "lo_spn.mul"(%19, %20) : (f64, f64) -> f64
      %22 = "lo_spn.add"(%17, %21) : (f64, f64) -> f64
      %23 = "lo_spn.log"(%22) : (f64) -> f64
      "lo_spn.batch_write"(%arg1, %arg2, %23) <{transposed = true}> : (memref<1x?xf64>, index, f64) -> ()
    }
    return
  }
// CHECK-LABEL:   func.func @spn_vector(
// CHECK-SAME:                          %[[VAL_0:.*]]: memref<?x6xf64>,
// CHECK-SAME:                          %[[VAL_1:.*]]: memref<1x?xf64>) {
// CHECK:           %[[VAL_2:.*]] = arith.constant 0 : index
// CHECK:           %[[VAL_3:.*]] = memref.dim %[[VAL_0]], %[[VAL_2]] : memref<?x6xf64>
// CHECK:           %[[VAL_4:.*]] = memref.alloc(%[[VAL_3]]) : memref<1x?xf64>
// CHECK:           call @vec_task_0(%[[VAL_0]], %[[VAL_4]]) : (memref<?x6xf64>, memref<1x?xf64>) -> ()
// CHECK:           %[[VAL_5:.*]] = bufferization.to_tensor %[[VAL_4]] : memref<1x?xf64>
// CHECK:           "lo_spn.copy"(%[[VAL_4]], %[[VAL_1]]) : (memref<1x?xf64>, memref<1x?xf64>) -> ()
// CHECK:           "lo_spn.return"() : () -> ()
// CHECK:         }
  func.func @spn_vector(%arg0: memref<?x6xf64>, %arg1: memref<1x?xf64>) {
    %c0 = arith.constant 0 : index
    %dim = memref.dim %arg0, %c0 : memref<?x6xf64>
    %alloc = memref.alloc(%dim) : memref<1x?xf64>
    call @vec_task_0(%arg0, %alloc) : (memref<?x6xf64>, memref<1x?xf64>) -> ()
    %0 = bufferization.to_tensor %alloc : memref<1x?xf64>
    %1 = bufferization.to_memref %0 : memref<1x?xf64>
    "lo_spn.copy"(%1, %arg1) : (memref<1x?xf64>, memref<1x?xf64>) -> ()
    "lo_spn.return"() : () -> ()
  }
}

